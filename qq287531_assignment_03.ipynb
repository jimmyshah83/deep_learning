{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1: Increase the skip window.  Looking at the training error and the closest words, does the model seem to get better or worse? Explain why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer 1:\n",
    "\n",
    "With a smaller skip window, the model is better suited to learn its neighbours and the target words context.\n",
    "Increasing the skip window would force the model to learn the context of a larger set of neighbours, which might not be related to the target word in the corpus, producing not so great results.\n",
    "\n",
    "Technically, a window size should not matter if we have a large dataset as it would learn the neighbours based on its occurance.\n",
    "\n",
    "In this case, increasing the window sizes to 50 and 500, it did perform slightly worse with a loss of ~ 5.19 as compared to a loss of ~ 4.69 with a skip window of 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2: Research and explain NCE loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer 2:\n",
    "\n",
    "The basic idea behind NCE is to convert a classification problem, to predict the next word, to a binary classification problem. That is, instead of using softmax to estimate a true probability distribution of the output word, a binary logistic regression is used where for each training sample, the classifier is fed a true pair and a number of randomly corrupted pairs and by learning to distinguish the true pairs from corrupted ones, the classifier will ultimately learn the word vectors.\n",
    "\n",
    "This is important: instead of predicting the next word, the binary classifier simply predicts whether a pair of words is good or bad.\n",
    "\n",
    "Word2Vec slightly customizes the process and calls it negative sampling. In Word2Vec, the words for the negative samples or the corrupted pairs, are drawn from a specially designed distribution, which favours less frequent words to be drawn more often."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 3: Why replace rare words with UNK rather than keeping them? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer 3:\n",
    "\n",
    "Mapping rare words to \"UNK\" simply means that we delete those words and replace them with the token \"UNK\" in \n",
    "the training data making our model unaware of any rare words in our corpus. This way, the model assumes that the token \"UNK\" will never actually occur in real data or better yet it ignores these tokens altogether."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 4: If you run the model more than once the t-SNE plot looks different each time.  Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer 4:\n",
    "\n",
    "t-SNE is a process to create 2 dimension maps from thousands of dimentions.\n",
    "\n",
    "The reason the t-SNE looks different each time the model is run is because the Word2Vec algorithm makes use of randomization internally while training and if training is spread over multiple threads, some additional order-of-presentation randomization is introduced. These mean that two runs, even in the exact same environment, can have different results.\n",
    "\n",
    "If the training is effective with sufficient data, appropriate parameters, enough training passes, etc all such models should be of similar quality when doing things like word similarity, even though the actual words will be in different places. There'll be some jitter in the relative rankings of words, but the results should be broadly similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 5: What happens to accuracy if you set vocabulary_size to 500?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer 5:\n",
    "\n",
    "A vocabulary is a dictionary of all the words in our corpus. If the vocabulary size is reduced to 500, the model would not be able to include all possible re-occuring words in our corpus and would only consider the top 500, making the other words in the corpus as \"UNK\" which would eventually result in worse results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 6: You may see antonyms like “less” and “more” next to each other in the t-SNE.  How does that make sense rather than them being at opposite ends of the plot?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer 6:\n",
    "\n",
    "Since word2vec is efficient in identifying the syntactic and symentic relationship between words, it would identify \"less\" and \"more\" having some sort of relation in terms of meaning and context. For instance, phrases such as \"more or less\" in the corpus would reveal that these two words are related. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
