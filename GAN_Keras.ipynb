{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GAN_Keras.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jimmyshah83/deep_learning/blob/master/GAN_Keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "M-t4DYsbVcoi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "GAN Using Keras"
      ]
    },
    {
      "metadata": {
        "id": "6_x4fervWMaA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 189
        },
        "outputId": "0a9df238-7961-47a3-f791-2cd96f4b5a46"
      },
      "cell_type": "code",
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PkRLPzdAgnIz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#import zipfile\n",
        "#import io\n",
        "#from zipfile import ZipFile\n",
        "#with ZipFile('drive/My Drive/cropped.zip', 'r') as zf:\n",
        "#   zf.extractall('drive/My Drive/images/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BK1GfOnCvmF4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "IMAGE_SIZE = 64\n",
        "NOISE_SIZE = 100\n",
        "LR_D = 0.00004\n",
        "LR_G = 0.0004\n",
        "BATCH_SIZE = 256\n",
        "EPOCHS = 1\n",
        "BETA1 = 0.5\n",
        "WEIGHT_INIT_STDDEV = 0.02\n",
        "EPSILON = 0.00005\n",
        "SAMPLES_TO_SHOW = 5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0LAXWqA7VhW5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d633e879-f7d7-47fa-8656-8adb27a76211"
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "import datetime\n",
        "import random\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import tensorflow as tf\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import *\n",
        "from keras.optimizers import Adam\n",
        "from keras.initializers import TruncatedNormal\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "obnF3b0NbXJ9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_images = np.asarray([np.asarray(Image.open(file).resize((IMAGE_SIZE, IMAGE_SIZE))) \n",
        "                           for file in glob('drive/My Drive/images/*.png')])\n",
        "\n",
        "print (\"Input: \" + str(train_images.shape))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e3y5nfP5Ephc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_images = (train_images - 127.5) / 127.5 # Normalize the images to [-1, 1]\n",
        "# Batch and shuffle the data\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(9877).batch(BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "a0v7XqdlfTjr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hpUMQej9XPgQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def make_generator_model():\n",
        "    model = Sequential()\n",
        "    \n",
        "    model.add(Dense(8*8*1024, use_bias=False, input_shape=(100,)))\n",
        "    model.add(Reshape((8, 8, 1024)))\n",
        "    model.add(LeakyReLU())\n",
        "    \n",
        "    model.add(Conv2DTranspose(512, (5, 5), padding='same', strides=2, kernel_initializer=TruncatedNormal(stddev=WEIGHT_INIT_STDDEV)))\n",
        "    model.add(BatchNormalization(epsilon=EPSILON))\n",
        "    model.add(LeakyReLU())\n",
        "    \n",
        "    \n",
        "    model.add(Conv2DTranspose(256, (5, 5), padding='same', strides=2, kernel_initializer=TruncatedNormal(stddev=WEIGHT_INIT_STDDEV)))\n",
        "    model.add(BatchNormalization(epsilon=EPSILON))\n",
        "    model.add(LeakyReLU())\n",
        "    \n",
        "    model.add(Conv2DTranspose(128, (5, 5), padding='same', strides=2, kernel_initializer=TruncatedNormal(stddev=WEIGHT_INIT_STDDEV)))\n",
        "    model.add(BatchNormalization(epsilon=EPSILON))\n",
        "    model.add(LeakyReLU())\n",
        "    \n",
        "    model.add(Conv2DTranspose(64, (5, 5), padding='same', strides=2, kernel_initializer=TruncatedNormal(stddev=WEIGHT_INIT_STDDEV)))\n",
        "    model.add(BatchNormalization(epsilon=EPSILON))\n",
        "    model.add(LeakyReLU())\n",
        "    \n",
        "    model.add(Conv2DTranspose(1, (5, 5), padding='same', strides=2, kernel_initializer=TruncatedNormal(stddev=WEIGHT_INIT_STDDEV)))\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Rio3jeUXclj6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "generator = make_generator_model()\n",
        "\n",
        "noise = tf.random.normal([1, 100])\n",
        "generated_image = generator(noise, training=False)\n",
        "\n",
        "plt.imshow(generated_image[0, :, :, 0], cmap='gray')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cwdlz2FXc6Y3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def make_discriminator_model():\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(64, (5, 5), padding='same', strides=2, kernel_initializer=TruncatedNormal(stddev=WEIGHT_INIT_STDDEV)))\n",
        "  model.add(BatchNormalization(epsilon=EPSILON))\n",
        "  model.add(LeakyReLU())\n",
        "    \n",
        "  model.add(Conv2D(128, (5, 5), padding='same', strides=2, kernel_initializer=TruncatedNormal(stddev=WEIGHT_INIT_STDDEV)))\n",
        "  model.add(BatchNormalization(epsilon=EPSILON))\n",
        "  model.add(LeakyReLU())\n",
        "    \n",
        "  model.add(Conv2D(256, (5, 5), padding='same', strides=2, kernel_initializer=TruncatedNormal(stddev=WEIGHT_INIT_STDDEV)))\n",
        "  model.add(BatchNormalization(epsilon=EPSILON))\n",
        "  model.add(LeakyReLU())\n",
        "              \n",
        "  model.add(Conv2D(514, (5, 5), padding='same', strides=2, kernel_initializer=TruncatedNormal(stddev=WEIGHT_INIT_STDDEV)))\n",
        "  model.add(BatchNormalization(epsilon=EPSILON))\n",
        "  model.add(LeakyReLU())\n",
        "              \n",
        "  model.add(Conv2D(1024, (5, 5), padding='same', strides=2, kernel_initializer=TruncatedNormal(stddev=WEIGHT_INIT_STDDEV)))\n",
        "  model.add(BatchNormalization(epsilon=EPSILON))\n",
        "  model.add(LeakyReLU())\n",
        "              \n",
        "  model.add(Reshape((8, 8, 1024), input_shape=(8*8*1024,)))\n",
        "  model.add(Dense(1))\n",
        "  model.add(Activation('sigmoid'))\n",
        "              \n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9xe6uEU_cmhc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "discriminator = make_discriminator_model()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mJ1sTmqaFFPd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FBGQXNIWFI9z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def discriminator_loss(real_output, fake_output):\n",
        "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "    total_loss = real_loss + fake_loss\n",
        "    return total_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ruZcyVInFLls",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def generator_loss(fake_output):\n",
        "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BLfAJs_XFMLe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "discriminator_optimizer = Adam(lr=LR_D, beta_1=BETA1)\n",
        "generator_optimizer = Adam(lr=LR_G, beta_1=BETA1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bVJNTVZ3FNwO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_step(images):\n",
        "    noise = tf.random.normal([BATCH_SIZE, NOISE_SIZE])\n",
        "\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "      generated_images = generator(noise, training=True)\n",
        "\n",
        "      real_output = discriminator(images, training=True)\n",
        "      fake_output = discriminator(generated_images, training=True)\n",
        "\n",
        "      gen_loss = generator_loss(fake_output)\n",
        "      disc_loss = discriminator_loss(real_output, fake_output)\n",
        "\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jUJbRBqvFPZ2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train(dataset):  \n",
        "  for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "    \n",
        "    for image_batch in dataset:\n",
        "      train_step(image_batch)\n",
        "    \n",
        "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ua78ng-_FQ9i",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train(train_dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}